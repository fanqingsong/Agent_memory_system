#!/usr/bin/env python3
"""
APIé…ç½®æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•ä¸åŒAPIæä¾›å•†çš„é…ç½®æ˜¯å¦æ­£ç¡®ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/test_api_config.py [provider]

æ”¯æŒçš„provider:
    - openai: æµ‹è¯•OpenAI API
    
    - azure: æµ‹è¯•Azure OpenAI API
"""

import asyncio
import os
import sys
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent_memory_system.utils.openai_client import LLMClient
from agent_memory_system.utils.config import config
from agent_memory_system.utils.logger import log

async def test_openai_api():
    """æµ‹è¯•OpenAI API"""
    print("ğŸ” æµ‹è¯•OpenAI APIé…ç½®...")
    
    if not config.llm.api_key:
        print("âŒ æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return False
    
    try:
        client = LLMClient(
            provider="openai",
            api_key=config.llm.api_key,
            api_base_url=config.llm.api_base_url,
            model=config.llm.model
        )
        
        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {config.llm.model}")
        if config.llm.api_base_url:
            print(f"âœ… APIåŸºç¡€URL: {config.llm.api_base_url}")
        
        # æµ‹è¯•APIè°ƒç”¨
        response = await client.chat_completion(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·ç”¨ç®€çŸ­çš„è¯å›å¤ã€‚",
            user_message="ä½ å¥½ï¼Œè¯·è¯´'æµ‹è¯•æˆåŠŸ'",
            temperature=0.1,
            max_tokens=50
        )
        
        print(f"âœ… APIè°ƒç”¨æˆåŠŸ: {response}")
        return True
        
    except Exception as e:
        print(f"âŒ OpenAI APIæµ‹è¯•å¤±è´¥: {e}")
        return False



async def test_azure_openai():
    """æµ‹è¯•Azure OpenAI API"""
    print("ğŸ” æµ‹è¯•Azure OpenAI APIé…ç½®...")
    
    if not config.llm.api_key:
        print("âŒ æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return False
    
    if not config.llm.api_base_url:
        print("âŒ æœªæ‰¾åˆ°OPENAI_API_BASE_URLç¯å¢ƒå˜é‡")
        return False
    
    try:
        client = LLMClient(
            provider="openai",
            api_key=config.llm.api_key,
            api_base_url=config.llm.api_base_url,
            model=config.llm.model
        )
        
        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {config.llm.model}")
        print(f"âœ… Azureç«¯ç‚¹: {config.llm.api_base_url}")
        
        # æµ‹è¯•APIè°ƒç”¨
        response = await client.chat_completion(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·ç”¨ç®€çŸ­çš„è¯å›å¤ã€‚",
            user_message="ä½ å¥½ï¼Œè¯·è¯´'æµ‹è¯•æˆåŠŸ'",
            temperature=0.1,
            max_tokens=50
        )
        
        print(f"âœ… APIè°ƒç”¨æˆåŠŸ: {response}")
        return True
        
    except Exception as e:
        print(f"âŒ Azure OpenAI APIæµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_embedding():
    """æµ‹è¯•åµŒå…¥ç”Ÿæˆ"""
    print("ğŸ” æµ‹è¯•åµŒå…¥ç”Ÿæˆ...")
    
    try:
        client = LLMClient(
            provider="openai",
            api_key=config.llm.api_key,
            api_base_url=config.llm.api_base_url
        )
        
        # æµ‹è¯•åµŒå…¥ç”Ÿæˆ
        text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        embedding = await client.create_embedding(text)
        
        print(f"âœ… åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        return True
        
    except Exception as e:
        print(f"âŒ åµŒå…¥ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_model_list():
    """æµ‹è¯•æ¨¡å‹åˆ—è¡¨è·å–"""
    print("ğŸ” æµ‹è¯•æ¨¡å‹åˆ—è¡¨è·å–...")
    
    try:
        client = LLMClient(
            provider="openai",
            api_key=config.llm.api_key,
            api_base_url=config.llm.api_base_url
        )
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        models = await client.list_models()
        
        print(f"âœ… è·å–åˆ° {len(models)} ä¸ªæ¨¡å‹:")
        for model in models[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {model}")
        if len(models) > 5:
            print(f"   ... è¿˜æœ‰ {len(models) - 5} ä¸ªæ¨¡å‹")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {e}")
        return False

def print_config_info():
    """æ‰“å°å½“å‰é…ç½®ä¿¡æ¯"""
    print("ğŸ“‹ å½“å‰é…ç½®ä¿¡æ¯:")
    print(f"   LLM Provider: {config.llm.provider}")
    print(f"   Model: {config.llm.model}")
    
    if config.llm.provider == "openai":
        print(f"   API Key: {'å·²è®¾ç½®' if config.llm.api_key else 'æœªè®¾ç½®'}")
        print(f"   API Base URL: {config.llm.api_base_url or 'ä½¿ç”¨é»˜è®¤'}")

    
    print()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ APIé…ç½®æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_config_info()
    
    # è·å–æµ‹è¯•ç±»å‹
    test_type = sys.argv[1] if len(sys.argv) > 1 else "all"
    
    results = []
    
    if test_type == "all":
        # æ ¹æ®å½“å‰é…ç½®æµ‹è¯•
        if config.llm.provider == "openai":
            if config.llm.api_base_url and "azure" in config.llm.api_base_url.lower():
                results.append(await test_azure_openai())
            else:
                results.append(await test_openai_api())

        
        # æµ‹è¯•é€šç”¨åŠŸèƒ½
        results.append(await test_embedding())
        results.append(await test_model_list())
        
    elif test_type == "openai":
        results.append(await test_openai_api())
        

        
    elif test_type == "azure":
        results.append(await test_azure_openai())
        
    elif test_type == "embedding":
        results.append(await test_embedding())
        
    elif test_type == "models":
        results.append(await test_model_list())
        
    else:
        print(f"âŒ æœªçŸ¥çš„æµ‹è¯•ç±»å‹: {test_type}")
        print("æ”¯æŒçš„æµ‹è¯•ç±»å‹: all, openai, azure, embedding, models")
        return
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœ:")
    
    if all(results):
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        failed_count = len([r for r in results if not r])
        print(f"   å¤±è´¥æµ‹è¯•æ•°: {failed_count}/{len(results)}")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1) 